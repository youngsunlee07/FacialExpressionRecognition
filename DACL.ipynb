{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6aafbfa-2d87-4f34-91fc-6228d87954b9",
   "metadata": {},
   "source": [
    "# DACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfbd0c8-48c1-4773-ad47-f0bf80f9b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea894a5b-fb50-4dab-b737-83efa4a0c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "        self.data = []\n",
    "\n",
    "        for label, emotion in enumerate(self.classes):\n",
    "            emotion_dir = os.path.join(root_dir, emotion)\n",
    "            for filename in os.listdir(emotion_dir):\n",
    "                img_path = os.path.join(emotion_dir, filename)\n",
    "                if os.path.exists(img_path):  \n",
    "                    self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        for _ in range(3):  \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('L')  \n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "            except FileNotFoundError:\n",
    "                time.sleep(1) \n",
    "        raise FileNotFoundError(f\"Failed to open {img_path} after multiple attempts.\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  \n",
    "    transforms.Resize((230, 230)),  \n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.RandomCrop(224, padding=8),  \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = FERDataset('C:/Users/user/Desktop/DLProject/FER-2013/train', transform=transform)\n",
    "test_dataset = FERDataset('C:/Users/user/Desktop/DLProject/FER-2013/test', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, height, width = x.size()\n",
    "        \n",
    "        # Generate attention weights\n",
    "        q = self.conv1(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
    "        k = self.conv1(x).view(batch_size, -1, height * width)\n",
    "        v = self.conv2(x).view(batch_size, C, height * width)\n",
    "\n",
    "        attn_weights = self.softmax(torch.bmm(q, k))\n",
    "        attn_output = torch.bmm(attn_weights, v.transpose(1, 2)).view(batch_size, C, height, width)\n",
    "\n",
    "        return x + attn_output\n",
    "\n",
    "class DACLModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DACLModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet18 model\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the first layer to accept 1-channel (grayscale) input\n",
    "        self.base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Remove the final FC layer\n",
    "        self.base_model.fc = nn.Identity()  \n",
    "        \n",
    "        self.attention = AttentionModule(in_channels=512)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = x.view(x.size(0), 512, 1, 1)  # Change shape to [batch_size, 512, 1, 1]\n",
    "        x = self.attention(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DACLModel(num_classes=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30], gamma=0.1)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training accuracy and loss after each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_loader):.4f}, Training Accuracy: {100.*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Appending labels and predictions for further evaluation\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "num_classes = 7\n",
    "all_labels_bin = label_binarize(all_labels, classes=[i for i in range(num_classes)])\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], np.array(all_outputs)[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "for i in range(num_classes):\n",
    "    print(f\"Class {i} AUC: {roc_auc[i]}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
